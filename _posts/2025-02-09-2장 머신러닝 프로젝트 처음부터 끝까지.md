---
# Header
title: "2ì¥. ë¨¸ì‹ ëŸ¬ë¡œì íŠ¸ ì²˜ìŒë¶€í„° ëê¹Œì§€"
excerpt: "2ì¥. ë¨¸ì‹ ëŸ¬ë¡œì íŠ¸ ì²˜ìŒë¶€í„° ëê¹Œì§€"
name: J
writer: J
categories: [ì±… ì •ë¦¬, í•¸ì¦ˆì˜¨ ë¨¸ì‹ ëŸ¬ë‹] # [ë©”ì¸ ì¹´í…Œê³ ë¦¬, ì„œë¸Œ ì¹´í…Œê³ ë¦¬]
tags:
  - []

toc: true
toc_sticky: true

date: 2025-02-09
last_modified_at: 2024-02-09

# --- ì•„ë˜ ë¶€í„° content
---

1. í° ê·¸ë¦¼ ë³´ê¸°.
2. ë°ì´í„° êµ¬í•˜ê¸°
3. ë°ì´í„°ë¡œë¶€í„° ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ê¸° ìœ„í•´ íƒìƒ‰í•˜ê³  ì‹œê°í™”í•˜ê¸°.
4. ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ìœ„í•´ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ê¸°.
5. ëª¨ë¸ì„ ì„ íƒí•˜ê³  í›ˆë ¨í•˜ê¸°.
6. ëª¨ë¸ì„ ë¯¸ì„¸ íŠœë‹í•˜ê¸°.
7. ì†”ë£¨ì…˜ ì œì‹œí•˜ê¸°.
8. ì‹œìŠ¤í…œì„ ë¡ ì¹­í•˜ê³ , ëª¨ë‹ˆí„°ë§í•˜ê³ , ìœ ì§€ ë³´ìˆ˜í•˜ê¸°.

# 2.1 ì‹¤ì œ ë°ì´í„°ë¡œ ì‘ì—…í•˜ê¸°

- StatLib ì €ì¥ì†Œì— ìˆëŠ” ì£¼íƒ ê°€ê²© ë°ì´í„°ì…‹!

# 2.2 í° ê·¸ë¦¼ ë³´ê¸°

- ìº˜ë¦¬í¬ë‹ˆì•„ ì¸êµ¬ ì¡°ì‚¬ ë°ì´í„°: (ìº˜ë¦¬í¬ë‹ˆì•„ì˜ ë¸”ë¡ ê·¸ë£¹ ë§ˆë‹¤) ì¸êµ¬, ì¤‘ê°„ ì†Œë“, ì¤‘ê°„ ì£¼íƒ ê°€ê²© ë“±ì„ í¬í•¨.

### 2.2.1 ë¬¸ì œ ì •ì˜

â“ "ë¹„ì§€ë‹ˆìŠ¤ì˜ ëª©ì ì´ ì •í™•íˆ ë¬´ì—‡ì¸ê°€"

![alt text](/assets/img_20250217/image-1.png)

> ğŸ’¡íŒŒì´í”„ë¼ì¸ì´ë€? <br> 
 ë°ì´í„° ì²˜ë¦¬ **ì»´í¬ë„ŒíŠ¸**ë“¤ì´ ì—°ì†ë˜ì–´ ìˆëŠ” ê²ƒ. ê° ì»´í¬ë„ŒíŠ¸ëŠ” ë§ì€ ë°ì´í„°ë¥¼ ì¶”ì¶œí•´ ì²˜ë¦¬í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ ë‹¤ë¥¸ ë°ì´í„° ì €ì¥ì†Œ(ì»´í¬ë„ŒíŠ¸ ì‚¬ì´ì˜ ì¸í„°í˜ì´ìŠ¤)ë¡œ ë³´ëƒ…ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ë‹¤ìŒ ì»´í¬ë„ŒíŠ¸ê°€ ê·¸ ë°ì´í„°ë¥¼ ì¶”ì¶œí•´ ìì‹ ì˜ ì¶œë ¥ ê²°ê³¼ë¥¼ ë§Œë“­ë‹ˆë‹¤.

â“ ì–´ë–»ê²Œ í•´ê²°í•  ê±´ì¸ê°€? 
- ì§€ë„ í•™ìŠµ -> ë ˆì´ë¸”ëœ í›ˆë ¨ ìƒ˜í”Œì´ ìˆê¸° ë•Œë¬¸.
  - ê·¸ ì¤‘ì—ì„œë„ íšŒê·€! (ë‹¤ì¤‘ íšŒê·€ ì´ì ë‹¨ë³€ëŸ‰ íšŒê·€)
- ë°°ì¹˜ í•™ìŠµ -> ë°ì´í„°ì— ì—°ì†ì ì¸ íë¦„ì´ ì—†ê³  ë¹ ë¥´ê²Œ ë³€í•˜ëŠ” ë°ì´í„°ì— ì ì‘í•˜ì§€ ì•Šì•„ë„ ë˜ê³ , ë°ì´í„°ê°€ ë©”ëª¨ë¦¬ì— ë“¤ì–´ê°ˆ ë§Œí¼ ì¶©ë¶„íˆ ì‘ê¸° ë•Œë¬¸.

### 2.2.2 ì„±ëŠ¥ ì¸¡ì • ì§€í‘œ ì„ íƒ

- í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨(RMSE)
  - Euclidean Norm ì´ìš© (L2 norm)

![alt text](/assets/img_20250217/image-2.png)

- í‰ê·  ì ˆëŒ€ ì˜¤ì°¨(MAE): ì´ìƒì¹˜ë¡œ ë³´ì´ëŠ” êµ¬ì—­ì´ ë§ì„ ì‹œ
  - Manhattan norm ì´ìš© (L1 norm)

> ğŸ’¡norm <br>
normì˜ ì§€ìˆ˜ê°€ í´ìˆ˜ë¡ í° ê°’ì˜ ì›ì†Œì— ì¹˜ìš°ì¹˜ë©° ì‘ì€ ê°’ì€ ë¬´ì‹œëœë‹¤. ê·¸ë˜ì„œ RMSEê°€ MAEë³´ë‹¤ ì¡°ê¸ˆ ë” ì´ìƒì¹˜ì— ë¯¼ê°í•˜ë‹¤. í•˜ì§€ë§Œ, ì´ìƒì¹˜ê°€ ë§¤ìš° ë“œë¬¼ë©´ RMSEê°€ ì˜ ë§ëŠ”ë‹¤.

### 2.2.3 ê°€ì • ê²€ì‚¬

â—ì£¼ì˜
- í•˜ìœ„ ì‹œìŠ¤í…œì—ì„œ ê°’ì„ ('ì €ë ´', 'ë³´í†µ', 'ê³ ê°€' ê°™ì€) ì¹´í…Œê³ ë¦¬ë¡œ ë°”ê¾¸ê³  ê°€ê²© ëŒ€ì‹  ì¹´í…Œê³ ë¦¬ë¥¼ ì´ìš©í•œë‹¤ë©´, ì •í™•í•œ ê°€ê²©ì„ êµ¬í•˜ëŠ”ê²Œ ì¤‘ìš”í•˜ì§€ ì•Šê²Œ ë˜ê³  ì´ëŸ¬í•œ ë¬¸ì œëŠ” íšŒê·€ê°€ ì•„ë‹ˆë¼ ë¶„ë¥˜ë‹¤.<br>
 => ì˜ ê³ ë ¤í•´ë³´ê¸°!!

# 2.3 ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

### 2.3.5 ë°ì´í„° ë‹¤ìš´ë¡œë“œ

- housing.csvë¥¼ ì••ì¶•í•œ housing.tgz íŒŒì¼.
- ë°ì´í„°ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´X -> ì½”ë“œë¡œ! (ìŠ¤ì¼€ì¥´ë§í•˜ì—¬ ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥!)

```py
from pathlib import Path
import pandas as pd
import tarfile
import urllib.request

def load_housing_data():
    tarball_path = Path("datasets/housing.tgz")
    if not tarball_path.is_file():
        Path("datasets").mkdir(parents=True, exist_ok=True)
        url = "https://github.com/ageron/data/raw/main/housing.tgz"
        urllib.request.urlretrieve(url, tarball_path)
        with tarfile.open(tarball_path) as housing_tarball:
            housing_tarball.extractall(path="datasets")
    return pd.read_csv(Path("datasets/housing/housing.csv"))

housing = load_housing_data()
```

### 2.3.6 ë°ì´í„° êµ¬ì¡° í›‘ì–´ë³´ê¸°

- .head() ì´ìš©
  - ê° í–‰ì€ í•˜ë‚˜ì˜ êµ¬ì—­ ë‚˜íƒ€ëƒ„.
- .info() ì´ìš©
  - ë°ì´í„°ì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª… ë³´ì—¬ì¤Œ.(ì „ì²´ í–‰ ìˆ˜, ê° íŠ¹ì„±ì˜ ë°ì´í„° íƒ€ì…ê³¼ ë„ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜ í™•ì¸)
- .value_counts(): ë³€ìˆ˜ë³„ ê°¯ìˆ˜
  - ex) housing['ocean_proximity'].value_counts()
- .describe(): ìˆ«ìí˜• íŠ¹ì„±ì˜ ìš”ì•½ ì •ë³´ ë³´ì—¬ì¤Œ
  - null ê°’ì€ ì œì™¸
  - count, max, min, std, ë°±ë¶„ìœ„ìˆ˜ ë“±
- .hist(bins= , figsize = ( , )): ìˆ«ìí˜• íŠ¹ì„±ì„ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ê·¸ë ¤ë³´ê¸°
  - ex) housing.hist(bins=50, figsize=(12,8))

### 2.3.7 í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë§Œë“¤ê¸°

- ë°ì´í„° ìŠ¤ëˆ„í•‘ í¸í–¥: ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ Data setìœ¼ë¡œ ì¼ë°˜í™” ì˜¤ì°¨ë¥¼ ì¶”ì •í•˜ë©´ ë§¤ìš° ë‚™ê´€ì ì¸ ì¶”ì •ì´ ë‚˜ì˜´.

- í…ŒìŠ¤íŠ¸ ì…‹ì€ ì¼ë°˜ì ìœ¼ë¡œ 20%ì •ë„ ë–¼ì–´ë†“ìœ¼ë©´ ë¨.

```py
import numpy as np

def shuffle_and_split_data(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    # indexë¥¼ ëœë¤ìœ¼ë¡œ ì„ê³ , ì •ë ¬í•´ì„œ ë°˜í™˜.
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]
```

â—ì´ ì½”ë“œë¥¼ ê³„ì† ì‹¤í–‰í•˜ë©´ ë‹¤ë¥¸ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ê°€ í˜•ì„±ë¨! <br>
1. ì²˜ìŒ ì‹¤í–‰ì—ì„œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ì €ì¥í•˜ê³ , ë‹¤ìŒë²ˆ ì‹¤í–‰ì—ì„œ ì´ë¥¼ ë¶ˆëŸ¬ë“¤ì´ëŠ” ê²ƒ.
2. ê°™ì€ ë‚œìˆ˜ ì¸ë±ìŠ¤ê°€ ìƒì„±ë˜ë„ë¡ np.random.permutation()ì„ í˜¸ì¶œí•˜ê¸° ì „ì— ë‚œìˆ˜ ë°œìƒê¸°ì˜ ì´ˆê¹ƒê°’ì„ ì§€ì •í•˜ëŠ” ê²ƒ.(np.random.seed(42))


> train_test_split
1. ë‚œìˆ˜ ì´ˆê¹ƒê°’ì„ ì§€ì •í•  ìˆ˜ ìˆëŠ” random_state ë§¤ê°œë³€ìˆ˜ê°€ ìˆë‹¤.
2. í–‰ì˜ ê°œìˆ˜ê°€ ê°™ì€ ì—¬ëŸ¬ ê°œì˜ ë°ì´í„°ì…‹ì„ ë„˜ê²¨ì„œ ë™ì¼í•œ ì¸ë±ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. (DFì´ ë ˆì´ë¸”ì— ë”°ë¼ ì—¬ëŸ¬ ê°œë¡œ ë‚˜ë‰˜ì–´ ìˆì„ ë•Œ ìœ ìš©)

```py
from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
```

> ê³„ì¸µì  ìƒ˜í”Œë§

- ì „ì²´ ì¸êµ¬ëŠ” ê³„ì¸µì´ë¼ëŠ” ë™ì§ˆì˜ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ë‰˜ê³ , í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ê°€ ì „ì²´ ì¸êµ¬ë¥¼ ëŒ€í‘œí•˜ë„ë¡ ê° ê³„ì¸µì—ì„œ ì˜¬ë°”ë¥¼ ìˆ˜ì˜ ìƒ˜í”Œì„ ì¶”ì¶œ.


â“ ë‚˜ìœ ìƒ˜í”Œì„ ì–»ì„ í™•ë¥  10.7%ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•
```py
from scipy.stats import binom

sample_size = 1000
ratio_female = 0.511
proba_too_small = binom(sample_size, ratio_female).cdf(485 - 1)
proba_too_large = 1 - binom(sample_size, ratio_female).cdf(535)
print(proba_too_small + proba_too_large)
```

â“ ì¹´í…Œê³ ë¦¬ë¡œ ë‚˜ëˆ„ê¸°

1. ë„ˆë¬´ ë§ì€ ê³„ì¸µìœ¼ë¡œ ë‚˜ëˆ„ë©´ ì•ˆë¨.
2. ê° ê³„ì¸µì´ ì¶©ë¶„íˆ ì»¤ì•¼í•¨.

```py
housing["income_cat"] = pd.cut(housing["median_income"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5])
```

â“ ë¶„í• ê¸°
- split()
  - í›ˆë ¨ê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìì²´ê°€ ì•„ë‹ˆë¼ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜.

```py
strat_train_set, strat_test_set = train_test_split(
    housing, test_size=0.2, stratify=housing["income_cat"], random_state=42)
```


# 2.4 ë°ì´í„° ì´í•´ë¥¼ ìœ„í•œ íƒìƒ‰ê³¼ ì‹œê°í™”

### 2.4.1 ì§€ë¦¬ì  ë°ì´í„° ì‹œê°í™”í•˜ê¸°

```py
housing.plot(kind="scatter", x="longitude", y="latitude", grid=True, alpha=0.2)
save_fig("better_visualization_plot")  # extra code
plt.show()
```
![alt text](/assets/img_20250217/image-3.png)

```py
housing.plot(kind="scatter", x="longitude", y="latitude", grid=True,
             s=housing["population"] / 100, label="population",
             c="median_house_value", cmap="jet", colorbar=True,
             legend=True, figsize=(10, 7))
save_fig("housing_prices_scatterplot")  # extra code
plt.show()
```
![alt text](/assets/img_20250217/image-4.png)

### 2.4.2 ìƒê´€ê´€ê³„ ì¡°ì‚¬í•˜ê¸°

- corr(): í‘œì¤€ ìƒê´€ê³„ìˆ˜ (í”¼ì–´ìŠ¨ì˜ r)

```py
corr_matrix = housing.corr(numeric_only=True)
```

```py
from pandas.plotting import scatter_matrix

attributes = ["median_house_value", "median_income", "total_rooms",
              "housing_median_age"]
scatter_matrix(housing[attributes], figsize=(12, 8))
save_fig("scatter_matrix_plot")  # ì¶”ê°€ ì½”ë“œ
plt.show()
```
![alt text](/assets/img_20250217/image-5.png)

â— ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê¼¬ë¦¬ê°€ ê¸´ ë°ì´í„°ëŠ” ë¡œê·¸ í•¨ìˆ˜ë‚˜ ì œê³±ê·¼ì„ ì´ìš©í•´ ë³€í˜•í•´ë„ ë¨.

### 2.4.3 íŠ¹ì„± ì¡°í•©ìœ¼ë¡œ ì‹¤í—˜í•˜ê¸°

```py
housing["rooms_per_house"] = housing["total_rooms"] / housing["households"]
housing["bedrooms_ratio"] = housing["total_bedrooms"] / housing["total_rooms"]
housing["people_per_house"] = housing["population"] / housing["households"]
```

- í•„ìš”í•´ ë³´ì´ëŠ” íŠ¹ì„± ë§Œë“¤ê¸°!

# 2.5 ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„

> ìˆ˜ë™ìœ¼ë¡œ í•˜ëŠ” ê²ƒ ëŒ€ì‹ , í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ìë™í™”í•´ì•¼ í•˜ëŠ” ì´ìœ 

1. ì–´ë–¤ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œë„ ë°ì´í„° ë³€í™˜ì„ ì†ì‰½ê²Œ ë°˜ë³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. í–¥í›„ í”„ë¡œì íŠ¸ì— ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë³€í™˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì ì§„ì ìœ¼ë¡œ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. ì‹¤ì œ ì‹œìŠ¤í…œì—ì„œ ì•Œê³ ë¦¬ì¦˜ì— ìƒˆ ë°ì´í„°ë¥¼ ì£¼ì…í•˜ê¸° ì „ì— ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
4. ì—¬ëŸ¬ ê°€ì§€ ë°ì´í„° ë³€í™˜ì„ ì‰½ê²Œ ì‹œë„í•´ë³¼ ìˆ˜ ìˆê³  ì–´ë–¤ ì¡°í•©ì´ ì¢‹ì€ì§€ í™•ì¸í•˜ëŠ”ë° í¸ë¦¬í•©ë‹ˆë‹¤.

```py
housing = strat_train_set.drop("median_house_value", axis=1)
# .drop(): ê¸°ë³¸ì ìœ¼ë¡œ ë°ì´í„° ë³µì‚¬ë³¸ì„ ë§Œë“¤ì–´ ë°˜í™˜í•˜ë©° strat_train_setì—ëŠ” ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤.
housing_labels = strat_train_set["median_house_value"].copy()
```

### 2.5.1 ë°ì´í„° ì •ì œ

1. í•´ë‹¹ êµ¬ì—­ ì œê±° (dropna)
2. ì „ì²´ íŠ¹ì„± ì‚­ì œ (drop)
3. ëˆ„ë½ëœ ê°’ ëŒ€ì²´ (fillna)

```py
housing.dropna(subset=["total_bedrooms"], inplace=True)    # ì˜µì…˜ 1

housing.drop("total_bedrooms", axis=1)                     # ì˜µì…˜ 2

median = housing["total_bedrooms"].median()                # ì˜µì…˜ 3
housing["total_bedrooms"].fillna(median, inplace=True)
```

> SimpleImputer í´ë˜ìŠ¤ ì‚¬ìš©

```py
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="median")

housing_num = housing.select_dtypes(include=[np.number])
# ìˆ«ì íƒ€ì…ì¸ ì—´ë§Œ ì¶”ì¶œ

imputer.fit(housing_num)
# ê° íŠ¹ì„±ì˜ ì¤‘ê°„ê°’ì„ ê³„ì‚°í•´ì„œ ê·¸ ê²°ê³¼ë¥¼ ê°ì²´ì˜ statistics_ ì†ì„±ì— ì €ì¥.

X = imputer.transform(housing_num)
```

â­ ëˆ„ë½ëœ ê°’ ëŒ€ì²´ ìœ„í•œ ë” ê°•ë ¥í•œ í´ë˜ìŠ¤ë“¤!<br>
1. most_frequent: ìµœë¹ˆê°’
2. constant: ìƒìˆ˜
3. KNNImputer: ëˆ„ë½ëœ ê°’ì„ ì´ íŠ¹ì„±ì— ëŒ€í•œ k-ìµœê·¼ì ‘ ì´ì›ƒì˜ í‰ê· ìœ¼ë¡œ ëŒ€ì²´. 
4. IterativeImputer: íŠ¹ì„±ë§ˆë‹¤ íšŒê·€ ëª¨ë¸ì„ í›ˆë ¨í•˜ì—¬ ë‹¤ë¥¸ ëª¨ë“  íŠ¹ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ëˆ„ë½ëœ ê°’ ì˜ˆì¸¡.

â­ ì‚¬ì´í‚·ëŸ°ì˜ ì„¤ê³„ ì² í•™<br>
- ì¼ê´€ì„±
  - ì¶”ì •ê¸°(estimator)
    - ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ ì¼ë ¨ì˜ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë“¤ì„ ì¶”ì •í•˜ëŠ” ê°ì²´.(imputer) ì¶”ì • ìì²´ëŠ” 
  fit() ë©”ì„œë“œì— ì˜í•´ ìˆ˜í–‰ë˜ê³  í•˜ë‚˜ì˜ ë§¤ê°œë³€ìˆ˜ë¡œ í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ë§Œ ì „ë‹¬.
  ì¶”ì • ê³¼ì •ì—ì„œ í•„ìš”í•œ ë‹¤ë¥¸ ë§¤ê°œë³€ìˆ˜ë“¤ì€ ëª¨ë‘ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ê°„ì£¼ë˜ê³ ,
  ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì €ì¥.
  - ë³€í™˜ê¸°(transformer)
    - ë°ì´í„°ì…‹ì„ ë³€í™˜í•˜ëŠ” ì¶”ì •ê¸°. ë³€í™˜ì€ imputerì˜ ê²½ìš°ì™€ ê°™ì´ í•™ìŠµëœ
  ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— ì˜í•´ ê²°ì •. ëª¨ë“  ë³€í™˜ê¸°ëŠ” fit_transformerë¥¼ ê°€ì§.
    - íŒë‹¤ìŠ¤ ë°ì´í„°í”„ë ˆì„ì´ ì…ë ¥ë˜ë”ë¼ë„ ë„˜íŒŒì´ ë°°ì—´ì„ ì¶œë ¥.
  - ì˜ˆì¸¡ê¸°(predictor)
    - ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ë°›ì•„ ì´ì— ìƒì‘í•˜ëŠ” ì˜ˆì¸¡ê°’ì„ ë°˜í™˜. ë˜í•œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼
    ì‚¬ìš©í•´ ì˜ˆì¸¡ì˜ í’ˆì§ˆì„ ì¸¡ì •í•˜ëŠ” score() ë©”ì„œë“œë¥¼ ê°€ì§.

- ê²€ì‚¬ ê°€ëŠ¥
- í´ë˜ìŠ¤ ë‚¨ìš© ë°©ì§€
- ì¡°í•©ì„±
- í•©ë¦¬ì ì¸ ê¸°ë³¸ê°’

### 2.5.2 í…ìŠ¤íŠ¸ì™€ ë²”ì£¼í˜• íŠ¹ì„± ë‹¤ë£¨ê¸°

> OrdinalEncoder í´ë˜ìŠ¤

```py
from sklearn.preprocessing import OrdinalEncoder

ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)
```

â—ë¬¸ì œ! <br>
- ê±°ë¦¬ì— ì˜ë¯¸ê°€ ë¶€ì—¬ë¨.(bad, goodê³¼ ê°™ì€ ê²½ìš°ì—ëŠ” ìƒê´€ì´ ì—†ëŠ”ë° ìœ„ì™€ ê°™ì€ ê²½ìš°ëŠ” ìƒê´€ O)

> ì›í•« ì¸ì½”ë”©
- í•œ íŠ¹ì„±ë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” 0.
- **ë”ë¯¸** íŠ¹ì„±: ìƒˆë¡œìš´ íŠ¹ì„±

```py
from sklearn.preprocessing import OneHotEncoder

cat_encoder = OneHotEncoder()
# OneHotEncoder(sparse=False)ë¡œ í•˜ë©´, ë„˜íŒŒì´ ë°°ì—´ ë°˜í™˜.
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
# OneHotEncoder()ì€ í¬ì†Œ í–‰ë ¬ì„ ë°˜í™˜.

housing_cat_1hot.toarray()
# toarray()ë¥¼ ì´ìš©í•´ ë°€ì§‘ ë°°ì—´ë¡œ ë°˜í™˜ ê°€ëŠ¥.
```

â­ get_dummies()<br>
- get_dummies()ëŠ” OneHotEncoderì™€ ë‹¤ë¥´ê²Œ ì¹´í…Œê³ ë¦¬ë¡œ í›ˆë ¨ë˜ì–´ìˆì§€
ì•ŠìŠµë‹ˆë‹¤. 
- ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ê°ì§€í•˜ë©´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤. (handle_unkown ë§¤ê°œë³€ìˆ˜ë¥¼ 'ignore'ë¡œ ì§€ì •í•˜ë©´
ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ê·¸ëƒ¥ 0ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)

â“ ì¹´í…Œê³ ë¦¬ íŠ¹ì„±ì´ ë„ˆë¬´ ë§ë‹¤ë©´?<br>
- ì´ íŠ¹ì„±ê³¼ ê´€ë ¨ëœ ìˆ«ìí˜• íŠ¹ì„±ìœ¼ë¡œ ë°”ê¾¸ê¸°! (ex. êµ­ê°€ ì½”ë“œëŠ” ì¸êµ¬ì™€
1ì¸ë‹¹ GDPë¡œ ë°”ê¿€ ìˆ˜ ìˆë‹¤.)
- category_encoders íŒ¨í‚¤ì§€ê°€ ì œê³µí•˜ëŠ” ì¸ì½”ë” ì¤‘ í•˜ë‚˜ ì´ìš© ê°€ëŠ¥.
- ì„ë² ë”©ì´ë¼ ë¶€ë¥´ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ì €ì°¨ì› ë²¡í„°ë¡œ ë°”ê¿€ ìˆ˜O (í‘œí˜„ í•™ìŠµì˜ í•œ ì˜ˆì‹œ)

### 2.5.3 íŠ¹ì„± ìŠ¤ì¼€ì¼ê³¼ ë³€í™˜

â—í›ˆë ¨ ì„¸íŠ¸ ì´ì™¸ì˜ ì–´ë–¤ ê²ƒì—ë„ fit()ì´ë‚˜ fit_transform() ë©”ì„œë“œë¥¼
ì‚¬ìš©í•´ì„œëŠ” ì•ˆëœë‹¤. í›ˆë ¨í•˜ê³  ë‚˜ë©´ ì´ë¥¼ ì‚¬ìš©í•´ ë‹¤ì–‘í•œ data setì— transform()
ë©”ì„œë“œë¥¼ ì ìš©í•  ìˆ˜ ìˆë‹¤.<br>
â—í›ˆë ¨ ì„¸íŠ¸ ê°’ì€ í•­ìƒ íŠ¹ì • ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§ë˜ì§€ë§Œ ìƒˆë¡œìš´ ë°ì´í„°ì— ì´ìƒì¹˜ê°€ ìˆë‹¤ë©´,
ì´ ë²”ìœ„ ë°–ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ ëœë‹¤. ì´ë¥¼ ì›ì¹˜ ì•Šìœ¼ë©´ MinMaxScalerì˜ clip ë§¤ê°œë³€ìˆ˜ë¥¼
Trueë¡œ ì§€ì •!

> ì…ë ¥ íŠ¹ì„± ë³€í™˜

- min-max ìŠ¤ì¼€ì¼ë§ (ì •ê·œí™”)

```py
from sklearn.preprocessing import MinMaxScaler

min_max_scaler = MinMaxScaler(feature_range=(-1, 1))
# feature_rangeë¡œ ë²”ìœ„ ì„¤ì •
housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)
```

- í‘œì¤€í™”
  - ì´ìƒì¹˜ì— ì˜í–¥ ëœ ë°›ìŒ.

```py
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
housing_num_std_scaled = std_scaler.fit_transform(housing_num)
```

â• ë©±ë²•ì¹™ ë¶„í¬ì²˜ëŸ¼ íŠ¹ì„± ë¶„í¬ì˜ ê¼¬ë¦¬ê°€ ì•„ì£¼ ê¸¸ê³  ë‘ê»ë‹¤ë©´?
- íŠ¹ì„±ì„ ë¡œê·¸ê°’ìœ¼ë¡œ ë°”ê¾¸ê¸°
- ë²„í‚·íƒ€ì´ì§• 
  - ë©€í‹°ëª¨ë‹¬ ë¶„í¬(ìµœëŒ“ê°’ì´ 2ê°œ ì´ìƒ ë‚˜íƒ€ë‚˜ëŠ” ë¶„í¬)
    - ì´ë•ŒëŠ” ë²„í‚· idë¥¼ ìˆ˜ì¹˜ê°€ ì•„ë‹Œ, ì¹´í…Œê³ ë¦¬ë¡œ ë‹¤ë£¸.(ë²„í‚· idë¥¼ OneHotEncoderë¥¼ ì´ìš©í•´ ì¸ì½”ë”© í•´ì•¼í•¨.)
    - ë©€í‹°ëª¨ë‹¬ ë¶„í¬ë¥¼ ë³€í™˜í•˜ëŠ” ë˜ë‹¤ë¥¸ ë°©ì•ˆ: ì¤‘ê°„ ì£¼íƒ ì—°ë„ì™€ íŠ¹ì • ëª¨ë“œ ì‚¬ì´ì˜ ìœ ì‚¬ë„ë¥¼
    ë‚˜íƒ€ë‚´ëŠ” íŠ¹ì„±ì„ ì¶”ê°€í•˜ëŠ” ê²ƒ! (by ë°©ì‚¬ ê¸°ì € í•¨ìˆ˜ - ì…ë ¥ ê°’ì´ ê³ ì • í¬ì¸íŠ¸ì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ ì¶œë ¥ê°’ì´ ì§€ìˆ˜ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” ê°€ìš°ìŠ¤ RBF)

    ```py
    from sklearn.metrics.pairwise import rbf_kernel

    age_simil_35 = rbf_kernel(housing[["housing_median_age"]], [[35]], gamma=0.1)
    # ê°ë§ˆ:ê¸°ì¤€ ê°’ì—ì„œ ë©€ì–´ì§ì— ë”°ë¼ ìœ ì‚¬ë„ ê°’ì´ ì–¼ë§ˆë‚˜ ë¹ ë¥´ê²Œ ê°ì†Œí•˜ë‚˜?
    ```
  - íŠ¹ì„±ê°’ì˜ ì—¬ëŸ¬ ë²”ì£¼ì— ëŒ€í•´ ë‹¤ì–‘í•œ ê·œì¹™ ì‰½ê²Œ í•™ìŠµ!

> íƒ€ê¹ƒê°’ ë³€í™˜

```py
from sklearn.linear_model import LinearRegression

target_scaler = StandardScaler()
scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())
# standardScalerëŠ” 2D ì…ë ¥ì„ ê¸°ëŒ€í•¨.

model = LinearRegression()
model.fit(housing[["median_income"]], scaled_labels)
some_new_data = housing[["median_income"]].iloc[:5]  # ìƒˆë¡œìš´ ë°ì´í„°ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤

scaled_predictions = model.predict(some_new_data)
predictions = target_scaler.inverse_transform(scaled_predictions)
# ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë˜ëŒë¦¼.
```

ë” ê°„ë‹¨í•˜ê²Œ!! TransformedTargetRegressor ì´ìš©!

```py
from sklearn.compose import TransformedTargetRegressor

model = TransformedTargetRegressor(LinearRegression(),
                                   transformer=StandardScaler())
model.fit(housing[["median_income"]], housing_labels)
predictions = model.predict(some_new_data)
# ì´ë•Œ predict() ë©”ì„œë“œì™€ inverse_transform() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ ìƒì„±.
```

### 2.5.4 ì‚¬ìš©ì ì •ì˜ ë³€í™˜ê¸°

1. ì–´ë–¤ í›ˆë ¨ë„ í•„ìš”í•˜ì§€ ì•ŠëŠ” ë³€í™˜.

```py
from sklearn.preprocessing import FunctionTransformer

log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)
# inverse_func ë§¤ê°œë³€ìˆ˜ëŠ” ì„ íƒì‚¬í•­. (ex. TransformedTargetRegressorì— ì´ ë³€í™˜ê¸°ë¥¼ ì´ìš©í•  ì˜ˆì •ì´ë©´ inverse_func ë§¤ê°œë³€ìˆ˜ì— ì—­ë³€ìˆ˜ í•¨ìˆ˜ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŒ)
log_pop = log_transformer.transform(housing[["population"]])
```
- ì¶”ê°€ì ì¸ ì¸ìˆ˜ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë°›ì„ ìˆ˜ ìˆìŒ.

```py
rbf_transformer = FunctionTransformer(rbf_kernel,
                                      kw_args=dict(Y=[[35.]], gamma=0.1))
age_simil_35 = rbf_transformer.transform(housing[["housing_median_age"]])
```

â— rbf ì»¤ë„ì€ ê³ ì • í¬ì¸íŠ¸ì—ì„œ ì¼ì • ê±°ë¦¬ë§Œí¼ ë–¨ì–´ì§„ ê°’ì´ í•­ìƒ 2ê°œ ì´ê¸°ì—, ì—­í•¨ìˆ˜ ì—†ìŒ.
<br>â—rbf ì»¤ë„ì€ íŠ¹ì„±ì„ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬X ë‘ ê°œì˜ íŠ¹ì„±ì„ ê°€ì§„ ë°°ì—´ì„ ì „ë‹¬í•˜ë©´ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ 2D ê±°ë¦¬(ìœ í´ë¦¬ë“œ ê±°ë¦¬)ë¥¼ ê³„ì‚°í•¨.

```py
sf_coords = 37.7749, -122.41
sf_transformer = FunctionTransformer(rbf_kernel,
                                     kw_args=dict(Y=[sf_coords], gamma=0.1))
sf_simil = sf_transformer.transform(housing[["latitude", "longitude"]])
```

2. íŠ¹ì„±ì„ í•©ì¹  ë•Œë„ ìœ ìš©í•˜ë‹¤. (FunctionTransformer)

```py
ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])
ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))
```

3. í›ˆë ¨ ê°€ëŠ¥í•œ ë³€í™˜ê¸°ê°€ í•„ìš”í•˜ë‹¤ë©´? (ë°ì´í„° ë¶„í¬ê°€ ë‹¤ë¥´ë©´ ê³¤ë€í•˜ê¸° ë•Œë¬¸)

- ì‚¬ì´í‚·ëŸ°ì€ **ë• íƒ€ì´í•‘**(ìƒì†ì´ë‚˜ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„ì´ ì•„ë‹ˆë¼ ê°ì²´ì˜ ì†ì„±ì´ë‚˜ ë©”ì„œë“œê°€ ê°ì²´ì˜ ìœ í˜•ì„ ê²°ì •í•˜ëŠ” ë°©ì‹)ì— ì˜ì¡´í•˜ê¸° ë•Œë¬¸ì— ì´ í´ë˜ìŠ¤ê°€ íŠ¹ì • í´ë˜ìŠ¤ë¥¼ ìƒì†í•  í•„ìš”X
- fit_transform() ë©”ì„œë“œëŠ” TransformerMixinì„ ìƒì†í•˜ë©´ ìë™ìœ¼ë¡œ ìƒì„±ëœë‹¤.
- BaseEstimatorë¥¼ ìƒì†í•˜ë©´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì— í•„ìš”í•œ ë‘ ë©”ì„œë“œ (get_params()ì™€ set_params())ë¥¼ ì¶”ê°€ë¡œ ì–»ê²Œë¨.

```py
# StandardScalerì™€ ë¹„ìŠ·í•˜ê²Œ ì‘ë™í•˜ëŠ” ì‚¬ìš©ì ì •ì˜ ë³€í™˜ê¸°

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_array, check_is_fitted

class StandardScalerClone(BaseEstimator, TransformerMixin):
    def __init__(self, with_mean=True):  # *argsë‚˜ **kwargsë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!
        self.with_mean = with_mean

    def fit(self, X, y=None):  # ì‚¬ìš©í•˜ì§€ ì•Šë”ë¼ë„ yë¥¼ ë„£ì–´ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤
        X = check_array(X)  # Xê°€ ë¶€ë™ì†Œìˆ˜ì (ì‹¤ìˆ˜(ì†Œìˆ˜ì ì´ ìˆëŠ” ìˆ«ì)ë¥¼ ì €ì¥í•˜ëŠ” ë°°ì—´) ë°°ì—´ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤
        self.mean_ = X.mean(axis=0)
        self.scale_ = X.std(axis=0)
        self.n_features_in_ = X.shape[1]  # ëª¨ë“  ì¶”ì •ê¸°ëŠ” fit()ì—ì„œ ì´ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        return self  # í•­ìƒ selfë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤!

    def transform(self, X):
        check_is_fitted(self)  # (í›ˆë ¨ìœ¼ë¡œ) í•™ìŠµëœ ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤
        X = check_array(X)
        assert self.n_features_in_ == X.shape[1]
        if self.with_mean:
            X = X - self.mean_
        return X / self.scale_
```

> ğŸ’¡ì£¼ì˜ ì‚¬í•­
- Sklearn.utils.validation íŒ¨í‚¤ì§€ì—ëŠ” ì…ë ¥ì„ ê²€ì¦í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ê°€ ì—¬ëŸ¬ ê°œ ìˆë‹¤.
- ì‚¬ì´í‚·ëŸ° íŒŒì´í”„ë¼ì¸ì€ Xì™€ y ë‘ ê°œì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ ë©”ì„œë“œê°€ í•„ìš”í•˜ë‹¤. yë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ, y=Noneì´ í•„ìš”!
- ëª¨ë“  ì¶”ì •ê¸°ëŠ” fit()ì— n_features_in_ì„ ì„¤ì •í•˜ê³  transform()ì´ë‚˜ predict()ë¥¼ ìˆ˜í–‰í• ë•Œ íŠ¹ì„± ê°œìˆ˜ê°€ ê°™ì€ì§€ í™•ì¸!
- fit() ë©”ì„œë“œëŠ” self ë°˜í™˜!
- ëª¨ë“  ì¶”ì •ê¸°ëŠ” dfì´ ì „ë‹¬ë  ë•Œ fit() ì•ˆì—ì„œ feature_names_inì„ ì„¤ì •í•´ì•¼í•œë‹¤. ë˜í•œ ëª¨ë“  ë³€í™˜ê¸°ëŠ” get_feature_names_out() ë©”ì„œë“œì™€ ì—­ë³€í™˜ì„ ìœ„í•œ inverse_transform() ë©”ì„œë“œë¥¼ ì œê³µí•´ì•¼ í•œë‹¤.

4. í•˜ë‚˜ì˜ ì‚¬ìš©ì ë³€í™˜ê¸°ëŠ” êµ¬í˜„ ì•ˆì—ì„œ ë‹¤ë¥¸ ì¶”ì •ê¸° ì´ìš© ê°€ëŠ¥.

```py
# fit() ì•ˆì—ì„œ í›ˆë ¨ ë°ì´í„°ì— ìˆëŠ” í•µì‹¬ í´ëŸ¬ìŠ¤í„°ë¥¼ ì‹ë³„ í•˜ê¸° ìœ„í•´ KMeans í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©
# transform() ì•ˆì—ì„œ rbf_kernel()ì„ ì´ìš©í•´ ê° ìƒ˜í”Œì´ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ê³¼ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ ì¸¡ì •
from sklearn.cluster import KMeans

class ClusterSimilarity(BaseEstimator, TransformerMixin):
    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):
        self.n_clusters = n_clusters
        self.gamma = gamma
        self.random_state = random_state

    def fit(self, X, y=None, sample_weight=None):
        # ì‚¬ì´í‚·ëŸ° 1.2ë²„ì „ì—ì„œ ìµœìƒì˜ ê²°ê³¼ë¥¼ ì°¾ê¸° ìœ„í•´ ë°˜ë³µí•˜ëŠ” íšŸìˆ˜ë¥¼ ì§€ì •í•˜ëŠ” `n_init` ë§¤ê°œë³€ìˆ˜ ê°’ì— `'auto'`ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.
        # `n_init='auto'`ë¡œ ì§€ì •í•˜ë©´ ì´ˆê¸°í™” ë°©ë²•ì„ ì§€ì •í•˜ëŠ” `init='random'`ì¼ ë•Œ 10, `init='k-means++'`ì¼ ë•Œ 1ì´ ë©ë‹ˆë‹¤.
        # ì‚¬ì´í‚·ëŸ° 1.4ë²„ì „ì—ì„œ `n_init`ì˜ ê¸°ë³¸ê°’ì´ 10ì—ì„œ `'auto'`ë¡œ ë°”ë€ë‹ˆë‹¤. ê²½ê³ ë¥¼ í”¼í•˜ê¸° ìœ„í•´ `n_init=10`ìœ¼ë¡œ ì§€ì •í•©ë‹ˆë‹¤.
        self.kmeans_ = KMeans(self.n_clusters, n_init=10, random_state=self.random_state)
        self.kmeans_.fit(X, sample_weight=sample_weight)
        return self  # í•­ìƒ selfë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤!

    def transform(self, X):
        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)

    def get_feature_names_out(self, names=None):
        return [f"í´ëŸ¬ìŠ¤í„° {i} ìœ ì‚¬ë„" for i in range(self.n_clusters)]

```

### 2.5.5 ë³€í™˜ íŒŒì´í”„ë¼ì¸

- Pipeline í´ë˜ìŠ¤ ì œê³µ
  - (ì´ë¦„/ì¶”ì •ê¸°) ìŒì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ìŒ.
  - ì¶”ì •ê¸°ëŠ” ë§ˆì§€ë§‰ì„ ì œì™¸í•˜ê³  ëª¨ë‘ ë³€í™˜ê¸°ì—¬ì•¼í•¨.
    - ë§ˆì§€ë§‰ ì¶”ì •ê¸°ëŠ” ë³€í™˜ê¸°, ì˜ˆì¸¡ê¸°ë¶€í„° ë‹¤ë¥¸ íƒ€ì…ì˜ ì¶”ì •ê¸°ê¹Œì§€ ëª¨ë‘ ê°€ëŠ¥!!
  - íŒŒì´í”„ë¼ì¸ì€ ì¸ë±ì‹±ì„ ì§€ì›í•¨. (ex.pipline[1], num_pipeline.named_steps["simpleimputer"])
  - íŒŒë¼ë¯¸í„° ì¡°ì • ê°€ëŠ¥ (ex.num_pipeline.set_params(simpleimputer__strategy="median"))

```py
# ìˆ˜ì¹˜ íŠ¹ì„±ì—ì„œ ëˆ„ë½ëœ ê°’ì„ ëŒ€ì²´í•˜ê³  ìŠ¤ì¼€ì¼ì„ ì¡°ì •
from sklearn.pipeline import Pipeline

num_pipeline = Pipeline([
    ("impute", SimpleImputer(strategy="median")),
    ("standardize", StandardScaler()),
])

# ì´ë¦„ ì§“ëŠ”ê²Œ ê·€ì°®ë‹¤ë©´?
# from sklearn.pipeline import make_pipeline
# num_pipeline = make_pipeline(SimpleImputer(strategy="median"), StandardScaler())
```

â­ì£¼í”¼í„° ë…¸íŠ¸ë¶ì—ì„œ import sklearnê³¼ sklearn.set_config(display='diagram
)ì„ ì‹¤í–‰í•˜ë©´ ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ í‘œí˜„!
```py
from sklearn import set_config

set_config(display='diagram')

num_pipeline
```

- ColumnTransformer ì´ìš©
  - í•˜ë‚˜ì˜ ë³€í™˜ê¸°ë¡œ ê° ì—´ë§ˆë‹¤ ì ì ˆí•œ ë³€í™˜ ì ìš© ê°€ëŠ¥.

```py
# num_pipeline - ìˆ˜ì¹˜í˜• / cat_pipeline - ë²”ì£¼í˜•
from sklearn.compose import ColumnTransformer

num_attribs = ["longitude", "latitude", "housing_median_age", "total_rooms",
               "total_bedrooms", "population", "households", "median_income"]
cat_attribs = ["ocean_proximity"]

cat_pipeline = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"))

preprocessing = ColumnTransformer([
    ("num", num_pipeline, num_attribs),
    ("cat", cat_pipeline, cat_attribs),
])
```

attributesë¥¼ ì´ë ‡ê²Œ ì§€ì •í•˜ëŠ”ê²ƒì´ ê·€ì°®ë‹¤ë©´?<br>
=> make_column_selector í´ë˜ìŠ¤ ì´ìš©!!

```py
from sklearn.compose import make_column_selector, make_column_transformer

preprocessing = make_column_transformer(
  # ë³€í™˜ê¸°ì— ì´ë¦„ ì§“ëŠ”ê²ƒì´ ê·€ì°®ì„ ë•Œ ì´ìš©!
    (num_pipeline, make_column_selector(dtype_include=np.number)),
    (cat_pipeline, make_column_selector(dtype_include=object)),
)
```

â­í¬ì†Œ í–‰ë ¬ê³¼ ë°€ì§‘ í–‰ë ¬ì´ ì„ì—¬ ìˆì„ ë•Œ ColumnTransformerëŠ” ìµœì¢… í–‰ë ¬ì˜ ë°€ì§‘ ì •ë„ë¥¼ ì¶”ì •. ë°€ì§‘ë„ê°€ ì„ê³„ê°’(ê¸°ë³¸ì ìœ¼ë¡œ
sparse_threshold=0.3 ì´ë‹¤)ë³´ë‹¤ ë‚®ìœ¼ë©´ í¬ì†Œ í–‰ë ¬ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

> ìµœì¢… íŒŒì´í”„ë¼ì¸ ë§Œë“¤ê¸°!!

ğŸ’¡ì¡°ê±´
1. ëˆ„ë½ëœ ê°’ -> ì¤‘ê°„ê°’ìœ¼ë¡œ ëŒ€ì²´!
2. ë²”ì£¼í˜• íŠ¹ì„±ì„ ì›-í•« ì¸ì½”ë”©!
3. ë¹„ìœ¨ íŠ¹ì„±ì„ ê³„ì‚°í•˜ì—¬ ì¶”ê°€.
4. í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„ ì¸¡ì •.
5. ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì€ ê· ë“± ë¶„í¬ë‚˜ ê°€ìš°ìŠ¤ ë¶„í¬ì— ê°€ê¹Œìš´ íŠ¹ì„±ì„ ì„ í˜¸í•˜ê¸° ë•Œë¬¸ì— ê¼¬ë¦¬ê°€ ë‘êº¼ìš´ ë¶„í¬ë¥¼ ë ëŠ”
íŠ¹ì„±ì„ ë¡œê·¸ê°’ìœ¼ë¡œ ë³€ê²½!
6. í‘œì¤€í™”.

```py
def column_ratio(X):
    return X[:, [0]] / X[:, [1]]

def ratio_name(function_transformer, feature_names_in):
    return ["ratio"]  # get_feature_names_outì— ì‚¬ìš©

def ratio_pipeline():
    return make_pipeline(
        SimpleImputer(strategy="median"),
        FunctionTransformer(column_ratio, feature_names_out=ratio_name),
        StandardScaler())

log_pipeline = make_pipeline(
    SimpleImputer(strategy="median"),
    FunctionTransformer(np.log, feature_names_out="one-to-one"),
    StandardScaler())
cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)
default_num_pipeline = make_pipeline(SimpleImputer(strategy="median"),
                                     StandardScaler())
preprocessing = ColumnTransformer([
        ("bedrooms", ratio_pipeline(), ["total_bedrooms", "total_rooms"]),
        ("rooms_per_house", ratio_pipeline(), ["total_rooms", "households"]),
        ("people_per_house", ratio_pipeline(), ["population", "households"]),
        ("log", log_pipeline, ["total_bedrooms", "total_rooms", "population",
                               "households", "median_income"]),
        ("geo", cluster_simil, ["latitude", "longitude"]),
        ("cat", cat_pipeline, make_column_selector(dtype_include=object)),
    ],
    remainder=default_num_pipeline)  # ë‚¨ì€ íŠ¹ì„±: housing_median_age

housing_prepared = preprocessing.fit_transform(housing)
housing_prepared.shape
preprocessing.get_feature_names_out()
```

# 2.6 ëª¨ë¸ ì„ íƒê³¼ í›ˆë ¨

> ì™„ë£Œí•œ ê²ƒ!

1. í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë‚˜ëˆ„ê¸°
2. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‘ì„±

### 2.6.1 í›ˆë ¨ ì„¸íŠ¸ì—ì„œ í›ˆë ¨í•˜ê³  í‰ê°€í•˜ê¸°

> í›ˆë ¨í•˜ê¸°1 (ì„ í˜•íšŒê·€ ëª¨ë¸)

```py
from sklearn.linear_model import LinearRegression

lin_reg = make_pipeline(preprocessing, LinearRegression())
lin_reg.fit(housing, housing_labels)

housing_predictions = lin_reg.predict(housing)
housing_predictions[:5].round(-2)  # -2 = ì‹­ì˜ ìë¦¬ì—ì„œ ë°˜ì˜¬ë¦¼
```

> í‰ê°€í•˜ê¸°(RMSE)
- mean_squared_error() í•¨ìˆ˜ ì‚¬ìš©

```py
from sklearn.metrics import mean_squared_error

lin_rmse = mean_squared_error(housing_labels, housing_predictions,
                              squared=False)
lin_rmse
# np.float64(65778.48225643061)
# ê³¼ì†Œì í•©!!
```

> í›ˆë ¨í•˜ê¸°2 (ê²°ì • íŠ¸ë¦¬)

```py
from sklearn.tree import DecisionTreeRegressor

tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))
tree_reg.fit(housing, housing_labels)
```

> í‰ê°€í•˜ê¸° (RMSE)
```py
housing_predictions = tree_reg.predict(housing)
tree_rmse = mean_squared_error(housing_labels, housing_predictions,
                              squared=False)
tree_rmse
# 0.0
```

ì—¥? ì´ìƒí•œë°? í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ëŠ” ì•„ì§ ì´ìš©í•˜ë©´ ì•ˆë˜ê¸° ë•Œë¬¸ì—, í›ˆë ¨ ì„¸íŠ¸ì˜ ì¼ë¶€ë¶„ìœ¼ë¡œ í›ˆë ¨í•˜ê³ 
ë‹¤ë¥¸ ì¼ë¶€ë¶„ìœ¼ë¡œ ëª¨ë¸ ê²€ì¦!!

### 2.6.2 êµì°¨ ê²€ì¦ìœ¼ë¡œ í‰ê°€í•˜ê¸°

- K-í´ë“œ êµì°¨ ê²€ì¦ ì´ìš©!

> ê²°ì • íŠ¸ë¦¬

```py
from sklearn.model_selection import cross_val_score

tree_rmses = -cross_val_score(tree_reg, housing, housing_labels,
                              scoring="neg_root_mean_squared_error", cv=10)
# ì‚¬ì´í‚·ëŸ°ì˜ êµì°¨ ê²€ì¦ ê¸°ëŠ¥ì€ scoring ë§¤ê°œë³€ìˆ˜ì— íš¨ìš© í•¨ìˆ˜ ê¸°ëŒ€! ê·¸ë˜ì„œ RMSEì˜ ìŒìˆ«ê°’ì¸ neg_mean_squared_error ì´ìš©            
```

```py
pd.Series(tree_rmses).describe()
# í‰ê·  RMSE: 66,868 / í‘œì¤€ í¸ì°¨: 2,061
# ê³¼ëŒ€ì í•©!
```

> RandomForestRegressor

- íŠ¹ì„±ì„ ëœë¤ìœ¼ë¡œ ì„ íƒí•´ì„œ ë§ì€ ê²°ì • íŠ¸ë¦¬ë¥¼ ë§Œë“¤ê³  ì˜ˆì¸¡ì˜ í‰ê· ì„ êµ¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë™.
- ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ë“¤ë¡œ êµ¬ì„±ëœ ì´ëŸ° ëª¨ë¸ì„ **ì•™ìƒë¸”**ì´ë¼ê³  í•œë‹¤.

```py
from sklearn.ensemble import RandomForestRegressor

forest_reg = make_pipeline(preprocessing,
                           RandomForestRegressor(random_state=42))
forest_rmses = -cross_val_score(forest_reg, housing, housing_labels,
                                scoring="neg_root_mean_squared_error", cv=10)
pd.Series(forest_rmses).describe()
# mean     45712.814157

forest_reg.fit(housing, housing_labels)
housing_predictions = forest_reg.predict(housing)
forest_rmse = mean_squared_error(housing_labels, housing_predictions,
                                 squared=False)
forest_rmse
# 16952
```

ê³¼ëŒ€ì í•©!!<br>
=> í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ì— ë§ì€ ì‹œê°„ì„ ë“¤ì´ê¸° ì „ì—, ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸(ë‹¤ì–‘í•œ ì»¤ë„ì˜ SVM, ì‹ ê²½ë§ ë“±) ì‹œë„!

# 2.7 ëª¨ë¸ ë¯¸ì„¸ íŠœë‹

â¡ï¸ê°€ëŠ¥ì„± ìˆëŠ” ëª¨ë¸ë“¤ì„ ì¶”ë ¸ë‹¤ê³  ê°€ì •!

### 2.7.1 ê·¸ë¦¬ë“œ ì„œì¹˜

- ì‚¬ì´í‚·ëŸ°ì˜ GridSearchCV ì´ìš©.
  - íŒŒì´í”„ë¼ì¸ì´ë‚˜ ColumnTransformerê°€ ì¶”ì •ê¸°ë¥¼ ê²¹ê²¹ì´ ê°ì‹¸ê³  ìˆë”ë¼ë„ ì¶”ì •ê¸°ì˜ ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì •í•  ìˆ˜ ìˆë‹¤.
     - ì´ì¤‘ ë°‘ì¤„ ë¬¸ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê³  ì°¨ë¡€ë¡œ ì°¾ìŒ!
  - GridSearchCVê°€ ê¸°ë³¸ê°’ì¸ refit=Trueë¡œ ì´ˆê¸°í™”ë˜ì—ˆë‹¤ë©´, êµì°¨ ê²€ì¦ìœ¼ë¡œ ìµœì ì˜ ì¶”ì •ê¸°ë¥¼ ì°¾ì€ ë‹¤ìŒ ì „ì²´ í›ˆë ¨ ì„¸íŠ¸ë¡œ ë‹¤ì‹œ í›ˆë ¨ì‹œí‚´.

```py
from sklearn.model_selection import GridSearchCV

full_pipeline = Pipeline([
    ("preprocessing", preprocessing),
    ("random_forest", RandomForestRegressor(random_state=42)),
])
param_grid = [
    {'preprocessing__geo__n_clusters': [5, 8, 10],
     'random_forest__max_features': [4, 6, 8]},
    {'preprocessing__geo__n_clusters': [10, 15],
     'random_forest__max_features': [6, 8, 10]},
]
grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,
                           scoring='neg_root_mean_squared_error')
grid_search.fit(housing, housing_labels)
```

```py
grid_search.best_params_
# ìµœìƒì˜ ì¡°í•© í™•ì¸
```

â­íŒŒì´í”„ë¼ì¸ ë³€í™˜ê¸°ë¥¼ í›ˆë ¨í•˜ëŠ”ë° ê³„ì‚° ë¹„ìš©ì´ ë§ì´ ë“ ë‹¤ë©´ íŒŒì´í”„ë¼ì¸ì˜ memory ë§¤ê°œë³€ìˆ˜ì— ìºì‹± ë””ë ‰í„°ë¦¬ ê²½ë¡œë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŒ. 

### 2.7.2 ëœë¤ ì„œì¹˜

- RandomizedSearchCV
  - í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ê³µê°„ì´ ì»¤ì§€ë©´ ìœ ìš©.
  - ê°€ëŠ¥í•œ ëª¨ë“  ì¡°í•© ì‹œë„X ê° ë°˜ë³µë§ˆë‹¤ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ì„ì˜ì˜ ìˆ˜ ëŒ€ì….
  - í•˜ì´í¼íŒŒë¼ë¯¸í„°ë§ˆë‹¤ ê°€ëŠ¥í•œ ê°’ì˜ ë¦¬ìŠ¤íŠ¸ë‚˜ í™•ë¥  ë¶„í¬ë¥¼ ì œê³µí•´ì•¼ í•¨.

```py
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50),
                  'random_forest__max_features': randint(low=2, high=20)}

rnd_search = RandomizedSearchCV(
    full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,
    scoring='neg_root_mean_squared_error', random_state=42)

rnd_search.fit(housing, housing_labels)
```

â•HalvingRandomSearchCVì™€ HalvingGridSearchCV<br>
- ì²« ë²ˆì§¸ ë°˜ë³µì—ì„œ ë§ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì´ ê·¸ë¦¬ë“œ ì„œì¹˜ë‚˜ ëœë¤ ì„œì¹˜ë¥¼ ì´ìš©í•´ ìƒì„±.
- ì´ í›„ë³´ë“¤ì„ ì‚¬ìš©í•´ í›ˆë ¨í•˜ê³  êµì°¨ ê²€ì¦ ì‚¬ìš©í•´ í‰ê°€. í•˜ì§€ë§Œ, ì²«ë²ˆì§¸ ë°˜ë³µì˜ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ì œí•œëœ ìì›(ì¼ë¶€ í›ˆë ¨ ì„¸íŠ¸ ì´ìš©)ìœ¼ë¡œ í›ˆë ¨
- ëª¨ë“  í›„ë³´ í‰ê°€ í›„, ìµœìƒì˜ í›„ë³´ë§Œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ ë” ë§ì€ ìì›ì„ ì´ìš©.
- ëª‡ë²ˆì˜ ë°˜ë³µí›„, ìµœì¢… í›„ë³´ë“¤ì´ ì „ì²´ ìì› ì‚¬ìš©í•´ í‰ê°€.

â•**ë³´ë„ˆìŠ¤ ì„¹ì…˜: í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìœ„í•œ ìƒ˜í”Œë§ ë¶„í¬ ì„ íƒ ë°©ë²•**

* `scipy.stats.randint(a, b+1)`: a~b ì‚¬ì´ì˜ _ì´ì‚°ì ì¸_ ê°’ì„ ê°€ì§„ í•˜ì´í¼íŒŒë¼ë¯¸í„°. ì´ ë²”ìœ„ì˜ ëª¨ë“  ê°’ì€ ë™ì¼í•œ í™•ë¥  ê°€ì§‘ë‹ˆë‹¤.
* `scipy.stats.uniform(a, b)`: ë§¤ìš° ë¹„ìŠ·í•˜ì§€ë§Œ _ì—°ì†ì ì¸_ íŒŒë¼ë¯¸í„°ì— ì‚¬ìš©í•©ë‹ˆë‹¤.
* `scipy.stats.geom(1 / scale)`: ì´ì‚°ì ì¸ ê°’ì˜ ê²½ìš° ì£¼ì–´ì§„ ìŠ¤ì¼€ì¼ ì•ˆì—ì„œ ìƒ˜í”Œë§í•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ scale=1000ì¸ ê²½ìš° ëŒ€ë¶€ë¶„ì˜ ìƒ˜í”Œì€ ì´ ë²”ì£¼ ì•ˆì— ìˆì§€ë§Œ ëª¨ë“  ìƒ˜í”Œ ì¤‘ 10% ì •ë„ëŠ” 100ë³´ë‹¤ ì‘ê³ , 10% ì •ë„ëŠ” 2300ë³´ë‹¤ í½ë‹ˆë‹¤.
* `scipy.stats.expon(scale)`: `geom`ì˜ ì—°ì†ì ì¸ ë²„ì „ì…ë‹ˆë‹¤. `scale`ì„ ê°€ì¥ ë§ì´ ë“±ì¥í•  ê°’ìœ¼ë¡œ ì§€ì •í•©ë‹ˆë‹¤.
* `scipy.stats.loguniform(a, b)`: í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì˜ ìŠ¤ì¼€ì¼ì„ ì–´ë–»ê²Œ ì§€ì •í• ì§€ ëª¨ë¥¼ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. a=0.01, b=100ìœ¼ë¡œ ì§€ì •í•˜ë©´ 0.01ê³¼ 0.1 ì‚¬ì´ì˜ ìƒ˜í”Œë§ê³¼ 10ê³¼ 100 ì‚¬ì´ì˜ ìƒ˜í”Œë§ ë¹„ìœ¨ì´ ë™ì¼í•©ë‹ˆë‹¤.

### 2.7.3 ì•™ìƒë¸” ê¸°ë²•

- ìµœìƒì˜ ëª¨ë¸ì„ ì—°ê²°í•´ ë³´ëŠ” ê²ƒ!
- ex) K-ìµœê·¼ì ‘ ì´ì›ƒ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ë¯¸ì„¸ íŠœë‹í•œ ë‹¤ìŒ ì´ ëª¨ë¸ì˜ ì˜ˆì¸¡ê³¼ ëœë¤ í¬ë ˆìŠ¤íŠ¸ì˜ ì˜ˆì¸¡ì„ í‰ê· í•˜ì—¬ ì˜ˆì¸¡ìœ¼ë¡œ ì‚¼ëŠ” ê²ƒ.

### 2.7.4 ìµœìƒì˜ ëª¨ë¸ê³¼ ì˜¤ì°¨ ë¶„ì„

- ìµœìƒì˜ ëª¨ë¸ì„ ë¶„ì„í•˜ë©´, ë¬¸ì œì— ëŒ€í•œ ì¢‹ì€ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ëŠ” ê²½ìš° ìˆìŒ.
  - ê° íŠ¹ì„±ì˜ ìƒëŒ€ì  ì¤‘ìš”ë„ë¥¼ ì•Œ ìˆ˜O

```py
final_model = rnd_search.best_estimator_  # ì „ì²˜ë¦¬ í¬í•¨ë¨
feature_importances = final_model["random_forest"].feature_importances_
feature_importances.round(2)
```

â­ sklearn.feature_selection.SelectFromModel ë³€í™˜ê¸°<br>
- ìë™ìœ¼ë¡œ ê°€ì¥ ëœ ìœ ìš©í•œ íŠ¹ì„±ì„ ì œê±°í•  ìˆ˜ ìˆìŒ.
- ì´ ë³€í™˜ê¸°ë¥¼ í›ˆë ¨í•˜ë©´, í•œ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³ , feature_importances_ ì†ì„±ì„ í™•ì¸í•˜ì—¬ ê°€ì¥ ìœ ìš©í•œ íŠ¹ì„±ì„ ì„ íƒ. ê·¸ë‹¤ìŒ transform() ë©”ì„œë“œë¥¼ í˜¸ì¶œí•  ë•Œ ë‹¤ë¥¸ íŠ¹ì„±ì„ ì‚­ì œ.

### 2.7.5 í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ì‹œìŠ¤í…œ í‰ê°€í•˜ê¸°

```py
X_test = strat_test_set.drop("median_house_value", axis=1)
y_test = strat_test_set["median_house_value"].copy()

final_predictions = final_model.predict(X_test)

final_rmse = mean_squared_error(y_test, final_predictions, squared=False)
print(final_rmse)
# 42537.741264526725
```

ì´ ì¶”ì •ê°’ì´ ì–¼ë§ˆë‚˜ ì •í™•í•œê°€?<br>
=> ì‹ ë¢°êµ¬ê°„ ì´ìš©!

```py
from scipy import stats

confidence = 0.95
squared_errors = (final_predictions - y_test) ** 2
np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,
                         loc=squared_errors.mean(),
                         scale=stats.sem(squared_errors)))
```                         
![alt text](/assets/img_20250217/image-6.png)

# 2.8 ë¡ ì¹­, ëª¨ë‹ˆí„°ë§, ì‹œìŠ¤í…œ ìœ ì§€ ë³´ìˆ˜

> ëª¨ë¸ ì €ì¥

- joblib ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ìš©

```py
import joblib

joblib.dump(final_model, "my_california_housing_model.pkl")
```
â— ì›í•˜ëŠ” ëª¨ë¸ë¡œ ì‰½ê²Œ ëŒì•„ì˜¬ ìˆ˜ ìˆë„ë¡ ì‹¤í—˜í•œ ëª¨ë“  ëª¨ë¸ì„ ì €ì¥í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. ê²€ì¦ ì ìˆ˜ì™€ ê²€ì¦ ì„¸íŠ¸ì— ëŒ€í•œ ì‹¤ì œ ì˜ˆì¸¡ë„ ì €ì¥ ê°€ëŠ¥.

> ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ

- ëª¨ë¸ì´ ì´ìš©í•˜ëŠ” ëª¨ë“  ì‚¬ìš©ì ì •ì˜ í´ë˜ìŠ¤ì™€ í•¨ìˆ˜ë¥¼ ë¨¼ì € ì„í¬íŠ¸ í•´ì•¼í•¨.

```py
import joblib

# ì¶”ê°€ ì½”ë“œ â€“ ì±…ì—ëŠ” ê°„ê²°í•¨ì„ ìœ„í•´ ì œì™¸í•¨
from sklearn.cluster import KMeans
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.metrics.pairwise import rbf_kernel

def column_ratio(X):
    return X[:, [0]] / X[:, [1]]

#class ClusterSimilarity(BaseEstimator, TransformerMixin):
#    [...]

final_model_reloaded = joblib.load("my_california_housing_model.pkl")

new_data = housing.iloc[:5]  # ìƒˆë¡œìš´ êµ¬ì—­ì´ë¼ ê°€ì •
predictions = final_model_reloaded.predict(new_data)
```

> ëª¨ë¸ì˜ ë°°í¬

- ì›¹ ì‚¬ì´íŠ¸ ì•ˆì—ì„œ ì´ìš© ê°€ëŠ¥
- ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ REST APIë¥¼ í†µí•´ ì§ˆì˜í•  ìˆ˜ ìˆëŠ” ì „ìš© ì›¹ ì„œë¹„ìŠ¤ë¡œ ëª¨ë¸ì„ ê°ìŒ€ ìˆ˜ ìˆìŒ.
- ëª¨ë¸ì„ êµ¬ê¸€ ë²„í…ìŠ¤ AIì™€ ê°™ì€ í´ë¼ìš°ë“œì— ë°°í¬í•˜ëŠ” ê²ƒ.
  - joblibì„ ì´ìš©í•´ ëª¨ë¸ ì €ì¥ -> êµ¬ê¸€ í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ì— ì—…ë¡œë“œ -> êµ¬ê¸€ ë²„í…ìŠ¤ AIë¡œ ì´ë™í•˜ì—¬ ìƒˆë¡œìš´ ëª¨ë¸ ë²„ì „ì„ ë§Œë“¤ê³  GCS íŒŒì¼ì„ ì§€ì •.
  - ë¡œë“œ ë°¸ëŸ°ì‹±ê³¼ ìë™ í™•ì¥ì„ ì²˜ë¦¬í•˜ëŠ” ì›¹ ì„œë¹„ìŠ¤ ë§Œë“¤ì–´ì§!

> ëª¨ë¸ ëª¨ë‹ˆí„°ë§

- ì§€ì†ì  ëª¨ë‹ˆí„°ë§ í•„ìš”.
- í›„ì† ì‹œìŠ¤í…œì˜ ì§€í‘œë¡œ ëª¨ë¸ ì„±ëŠ¥ ì¶”ì • ê°€ëŠ¥.
- ì‚¬ëŒì˜ ë¶„ì„ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ.

â¡ï¸ ë„ˆë¬´ ì¼ì´ ë§ì•„ ã…œã…œ ë”°ë¼ì„œ! ê°€ëŠ¥í•œ ë§ì€ ê²ƒì„ ìë™í™” í•´ì•¼í•¨!

> ìë™í™”

- ì •ê¸°ì ìœ¼ë¡œ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë ˆì´ë¸”ì„ ë‹¨ë‹¤.
- ëª¨ë¸ì„ í›ˆë ¨í•˜ê³ , ë¯¸ì„¸ íŠœë‹í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±.
- ì—…ë°ì´íŠ¸ëœ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ìƒˆë¡œìš´ ëª¨ë¸ê³¼ ì´ì „ ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë¥¼ í•˜ë‚˜ ë” ì‘ì„±!

> ë§Œë“  ëª¨ë“  ëª¨ë¸ ë°±ì—…!!

- ìƒˆë¡œìš´ ëª¨ë¸ì´ ì‘ë™í•˜ì§€ ì•Šì„ì‹œ, ì´ì „ ëª¨ë¸ë¡œ ë¹ ë¥´ê²Œ ë¡¤ë°±í•˜ê¸° ìœ„í•´!

â­**MLOPS** í•„ìš”!!

# ì§ì ‘ í•´ë³´ì„¸ìš”!!

>ğŸ’¡ì •ë¦¬<br>
ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ -> ëª¨ë‹ˆí„°ë§ ë„êµ¬ êµ¬ì¶• -> ì‚¬ëŒì˜ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì„¸íŒ… -> ì£¼ê¸°ì ì¸ ëª¨ë¸ í•™ìŠµ ìë™í™”

